{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import pymupdf\n",
    "\n",
    "def scrape_pdf(file_path):\n",
    "    text = \"\"\n",
    "    with open(file_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        for page_num in range(len(reader.pages)):\n",
    "            page = reader.pages[page_num]\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "\n",
    "def scrape_pdf_with_pymupdf(file_path):\n",
    "    text = \"\"\n",
    "    doc = pymupdf.open(file_path)  # open a document\n",
    "    for page in doc:  # iterate the document pages\n",
    "        text += page.get_text()  # get plain text encoded as UTF-8\n",
    "    return text\n",
    "\n",
    "# Example usage:\n",
    "file_path = 'data/example.pdf'\n",
    "extracted_text = scrape_pdf(file_path)\n",
    "print(extracted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_to_string(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Example usage:\n",
    "csv_content = read_csv_to_string('data/extracted_data.csv')\n",
    "print(csv_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import tiktoken\n",
    "\n",
    "def scrape_web_url(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        return soup.prettify()\n",
    "        # soup.prettify()\n",
    "    else:\n",
    "        return f\"Failed to retrieve the web page. Status code: {response.status_code}\"\n",
    "    \n",
    "# Function to remove tags\n",
    "def remove_tags(html):\n",
    "\n",
    "    # parse html content\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    for data in soup(['style', 'script']):\n",
    "        # Remove tags\n",
    "        data.decompose()\n",
    "\n",
    "    # return data by retrieving the tag content\n",
    "    return ' '.join(soup.stripped_strings)\n",
    "\n",
    "    \n",
    "response = scrape_web_url('https://nav.al/rich')\n",
    "response = remove_tags(response)\n",
    "# print(response.get_text().replace(' ', ''))\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "tokens = len(tokenizer.encode(response))\n",
    "print(tokens)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "\n",
    "    def start_requests(self):\n",
    "        urls = [\n",
    "            \"https://quotes.toscrape.com/page/1/\",\n",
    "            \"https://quotes.toscrape.com/page/2/\",\n",
    "        ]\n",
    "        for url in urls:\n",
    "            yield scrapy.Request(url=url, callback=self.parse)\n",
    "\n",
    "    def parse(self, response):\n",
    "        page = response.url.split(\"/\")[-2]\n",
    "        filename = f\"quotes-{page}.html\"\n",
    "        Path(filename).write_bytes(response.body)\n",
    "        self.log(f\"Saved file {filename}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
